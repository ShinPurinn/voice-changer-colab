{"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"colab":{"provenance":[],"private_outputs":true,"gpuType":"T4","include_colab_link":true},"accelerator":"GPU","kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30559,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/hinabl/public-w-okada-voice-changer?scriptVersionId=208520469\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"### w-okada's Voice Changer | **Google Colab** / **Kaggle**\r\n\r\n---\r\n\r\n##**READ ME - VERY IMPORTANT**\r\n\r\nThis is an attempt to run [Realtime Voice Changer](https://github.com/w-okada/voice-changer) on Google Colab and Kaggle, still not perfect but is totally usable, you can use the following settings for better results:\r\n\r\nIf you're using a index: `f0: RMVPE_ONNX | Chunk: 112 or higher | Extra: 8192`\\\r\nIf you're not using a index: `f0: RMVPE_ONNX | Chunk: 96 or higher | Extra: 16384`\\\r\n**Don't forget to select your GPU in the GPU field (<b>Tesla T4</b>, for free users on Colab | P100 on Kaggle)*\r\n> Seems that PTH models performance better than ONNX for now, you can still try ONNX models and see if it satisfies you\r\n\r\n\r\n*You can always [click here](https://rentry.co/VoiceChangerGuide#gpu-chart-for-known-working-chunkextra) to check if these settings are up-to-date*\r\n<br><br>\r\n\r\n---\r\n\r\n###Always use GPU (**VERY VERY VERY IMPORTANT!**)\r\nYou need to use a GPU so the Voice Changer can work faster and better\\\r\nUse the menu above and click on **Runtime** » **Change runtime** » **Hardware acceleration** to select a GPU (**T4 is the free one**)\r\n\r\n---\r\n\r\n# **Credits and Support**\r\nRealtime Voice Changer by [w-okada](https://github.com/w-okada)\\\r\nColab files updated by [rafacasari](https://github.com/Rafacasari)\\\r\nRecommended settings by [Blanc](https://github.com/Blanc-dot)\\\r\nModified again by [Hina](https://github.com/HinaBl)\\\r\nEnable FCPE by [TheTrustedComputer](https://github.com/TheTrustedComputer)\\\r\nAddition of HRZN [Nick088](https://github.com/Nick088Official)\r\n\r\nNeed help? [AI Hub Discord](https://discord.gg/aihub) » ***#help-realtime-vc***\r\n\r\n---","metadata":{"id":"Lbbmx_Vjl0zo"}},{"cell_type":"code","source":"#=================Updated=================\n# @title **[1]** Clone repository and install dependencies\n# @markdown This first step will download the latest version of Voice Changer and install the dependencies. **It can take some time to complete.**\nimport os\nimport time\nimport subprocess\nimport threading\nimport shutil\nimport base64\nimport codecs\n\n# Configs\nRun_Cell=0\n\n\n#@markdown ---\n# @title **[Optional]** Connect to Google Drive\n# @markdown Using Google Drive will automatically save your uploaded models for later use.\n\nUse_Drive=True #@param {type:\"boolean\"}\nnotebook_env=0\n# Check what platform the notebook is running on\nif os.path.exists('/content'):\n  notebook_env=1\n  print(\"Welcome to ColabMod\")\n  from google.colab import drive\n\nelif os.path.exists('/kaggle/working'):\n  notebook_env=2\n  print(\"Welcome to Kaggle Mod\")\nelse:\n  notebook_env=3\n  print(\"Welcome!\")\n\n\n\nexternalgit=codecs.decode('uggcf://tvguho.pbz/j-bxnqn/ibvpr-punatre.tvg','rot_13')\nrvctimer=codecs.decode('uggcf://tvguho.pbz/uvanoy/eipgvzre.tvg','rot_13')\npathloc=codecs.decode('ibvpr-punatre','rot_13')\n\nfrom IPython.display import clear_output, Javascript\n\ndef update_timer_and_print():\n    global timer\n    while True:\n        hours, remainder = divmod(timer, 3600)\n        minutes, seconds = divmod(remainder, 60)\n        timer_str = f'{hours:02}:{minutes:02}:{seconds:02}'\n        print(f'\\rTimer: {timer_str}', end='', flush=True)  # Print without a newline\n        time.sleep(1)\n        timer += 1\ntimer = 0\nthreading.Thread(target=update_timer_and_print, daemon=True).start()\n\n!pip install colorama --quiet\nfrom colorama import Fore, Style\n\nprint(f\"{Fore.CYAN}> Cloning the repository...{Style.RESET_ALL}\")\n\n\n\n\n!git clone --depth 1 $externalgit &> /dev/null\n%cd $pathloc\n\n# !git fetch --depth 1 origin 11672e965338c852aac6e17b0f724d86db07b7bb\n# !git reset --hard 11672e965338c852aac6e17b0f724d86db07b7bb\n# !git clean -df\n%cd ../\n\n\nif notebook_env==1:\n  if Use_Drive==True:\n    if not os.path.exists('/content/drive'):\n      drive.mount('/content/drive')\n\n      !mkdir -p /content/drive/MyDrive/voice-changer/server/model_dir\n      !rm -rf /content/voice-changer/server/model_dir\n\n      drive_dir = \"/content/drive/MyDrive/voice-changer/server/model_dir\"\n      colab_dir = \"/content/voice-changer/server/model_dir\"\n      time.sleep(5)\n\n      os.symlink(drive_dir,colab_dir,True)\n    # %cd /content/drive/MyDrive\nprint(f\"{Fore.GREEN}> Successfully cloned the repository!{Style.RESET_ALL}\")\n%cd $pathloc/server/\n\n\n#custom sub\nif notebook_env==1:\n  !sed -i \"s/-.-.-.-/Colab.Mod/\" '../client/demo/dist/assets/gui_settings/version.txt'\nelif notebook_env==2:\n  !sed -i \"s/-.-.-.-/Kaggle.Mod/\" '../client/demo/dist/assets/gui_settings/version.txt'\nelif notebook_env==3:\n  !sed -i \"s/-.-.-.-/Online.Mod/\" '../client/demo/dist/assets/gui_settings/version.txt'\nelse:\n  !sed -i \"s/-.-.-.-/Online.Mod/\" '../client/demo/dist/assets/gui_settings/version.txt'\n  print(\"Notebook Env Not Found\")\n\n\nprint(f\"{Fore.CYAN}> Installing libportaudio2...{Style.RESET_ALL}\")\n!apt-get -y install -qq libportaudio2 > /dev/null 2>&1\n!npm install -g @hrzn/cli > /dev/null 2>&1\n!sudo apt-get -qq update > /dev/null 2>&1\n!sudo apt-get install -qq portaudio19-dev -y > /dev/null 2>&1\n!apt install -qq psmisc > /dev/null 2>&1\n\n!sed -i '/torch==/d' requirements.txt\n\n!sed -i '/torchaudio==/d' requirements.txt\n\n!sed -i '/numpy==/d' requirements.txt\n\n# Enabled FCPE\n# !sed -i '/from voice_changer.RVC.pitchExtractor.RMVPEPitchExtractor import RMVPEPitchExtractor/a\\from voice_changer.RVC.pitchExtractor.FcpePitchExtractor import FcpePitchExtractor' voice_changer/RVC/pitchExtractor/PitchExtractorManager.py\n\nprint(f\"{Fore.CYAN}> Installing pre-dependencies...{Style.RESET_ALL}\")\n# Install dependencies that are missing from requirements.txt and pyngrok\n!pip install pip==23.3.1 --quiet\n!pip install faiss-gpu --quiet\n!pip install fairseq --quiet\n!pip install pyngrok --quiet\n!pip install pyworld --no-build-isolation --quiet\n# Install webstuff\nimport asyncio\nimport re\n!pip install gdown portpicker torchfcpe\nprint(f\"{Fore.CYAN}> Installing dependencies from requirements.txt...{Style.RESET_ALL}\")\n!pip install -r requirements.txt --quiet\nclear_output()\nRun_Cell=1\nprint(f\"{Fore.GREEN}> Successfully installed all packages!{Style.RESET_ALL}\")","metadata":{"id":"86wTFmqsNMnD","cellView":"form"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#@title **[Optional]** Upload a voice model (Run this before running the Voice Changer)\nimport os\nimport sys\nimport json\nimport requests\n\nmodel_slot = 0 # @param {type:\"number\"}\nmodel_slot = str(model_slot)\n\n!rm -rf model_dir/$model_slot\n#@markdown **[Optional]** Add an icon to the model\nicon_link = \"https://i.scdn.co/image/ab6761610000e5ebbab6de14398281b2d37bc0b5\" #@param {type:\"string\"}\nicon_link = '\"'+icon_link+'\"'\n!mkdir model_dir\n!mkdir model_dir/$model_slot\n#@markdown Put your model's download link here `(must be a zip file)` only supports **huggingface.co** & **google drive**<br>\nmodel_link = \"https://huggingface.co/HinaBl/Nekrolina/resolve/main/Nekorolina_510.zip?download=true\"  #@param {type:\"string\"}\n\nif model_link.startswith(\"https://www.weights.gg\") or model_link.startswith(\"https://weights.gg\"):\n  print(\"Links from weights.gg is no longer supported.\")\n  sys.exit()\nelif model_link.startswith(\"https://drive.google.com\"):\n  model_link = '\"'+model_link+'\"'\n  !gdown $model_link --fuzzy -O model.zip\n  print(\"Model from Drive\")\nelif model_link.startswith(\"https://huggingface.co\"):\n  model_link = model_link\n  model_link = '\"'+model_link+'\"'\n  !curl -L $model_link > model.zip\n  print(\"Model from hugginface Link\")\nelse:\n  model_link = model_link\n  model_link = '\"'+model_link+'\"'\n  !curl -L -O $model_link\n  !mv ./*.pth model_dir/$model_slot/\n  print('Model(.pth) or a direct model link.')\n\n# Conditionally set the iconFile based on whether icon_link is empty\nif icon_link == '\"\"':\n    iconFile = \"\"\n    print(\"icon_link is empty, so no icon file will be downloaded.\")\nelse:\n    iconFile = \"icon.png\"\n    !curl -L $icon_link > model_dir/$model_slot/icon.png\n\n\n!unzip model.zip -d model_dir/$model_slot\n\n!mv model_dir/$model_slot/*/* model_dir/$model_slot/\n!rm -rf model_dir/$model_slot/*/\n!rm -rf model.zip\n#@markdown **Model Voice Convertion Setting**\nTune = 12 #@param {type:\"slider\",min:-50,max:50,step:1}\nIndex = 0 #@param {type:\"slider\",min:0,max:1,step:0.1}\n\nparam_link = \"\"\nif param_link == \"\":\n  paramset = requests.get(\"https://pastebin.com/raw/SAKwUCt1\").text\n  exec(paramset)\n\nclear_output()\nprint(\"\\033[93mModel with the name of \"+model_name+\" has been Imported to slot \"+model_slot)","metadata":{"cellView":"form","id":"Arp4kTo8kSna"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import codecs\nimport subprocess, threading, time, socket, urllib.request, portpicker\nfrom IPython.display import clear_output, Javascript\nfrom IPython.display import Audio, display\nPORT = portpicker.pick_unused_port()\n#=======================Updated=========================\n\n# @title Start Server using **NGROK** or **HRZN**\n# @markdown This cell will start the server, the first time that you run it will download the models, so it can take a while (~1-2 minutes)\n\n#======================Tunnels===========================\n\nTUNNEL = \"HRZN\" #@param [\"NGROK\",\"HRZN\"]\n\n# @markdown ---\n# @markdown You'll need a ngrok or hrzn account, but <font color=green>**it's free**</font> and easy to create!\n# @markdown ---\n# @markdown **1** - Create a <font color=green>**free**</font> account at [ngrok](https://dashboard.ngrok.com/signup) / [hrzn](https://hrzn.run/login) or **login with Google/Github account**\\\n# @markdown **2** - If you didn't logged in with Google/Github, you will need to **verify your e-mail**!\\\n# @markdown **3** - Get your [ngrok](https://dashboard.ngrok.com/get-started/your-authtoken) or [hrzn](https://hrzn.run/dashboard) to get your auth token, and place it here:\nToken = 'NGROK | HRZN TOKEN' # @param {type:\"string\"}\n# @markdown **4** - *(optional for ngrok)* Change to a region near to you or keep at United States if increase latency\\\n# @markdown `Default Region: ap - Asia/Pacific (Singapore)`\nRegion = \"ap - Asia/Pacific (Singapore)\" # @param [\"ap - Asia/Pacific (Singapore)\", \"au - Australia (Sydney)\",\"eu - Europe (Frankfurt)\", \"in - India (Mumbai)\",\"jp - Japan (Tokyo)\",\"sa - South America (Sao Paulo)\", \"us - United States (Ohio)\"]\n\n#@markdown **5** - *(optional)* Other options:\nClearConsole = True  # @param {type:\"boolean\"}\nPlay_Notification = False  # @param {type:\"boolean\"}\n\n# ---------------------------------\n# DO NOT TOUCH ANYTHING DOWN BELOW!\n# ---------------------------------\n\n# Check if Run_Cell\nif 'Run_Cell' not in globals():\n    print(\"No, Go back to the first cell and run it\")\nelse:\n  if Run_Cell == 0:\n    print(\"No, Go back to the first cell and run it\")\n  else:\n    if TUNNEL == \"NGROK\":\n      from pyngrok import conf, ngrok\n      MyConfig = conf.PyngrokConfig()\n      MyConfig.auth_token = Token\n      MyConfig.region = Region[0:2]\n      #conf.get_default().authtoken = Token\n      #conf.get_default().region = Region\n      conf.set_default(MyConfig);\n\n\n\n\n      ngrokConnection = ngrok.connect(PORT)\n      public_url = ngrokConnection.public_url\n    elif TUNNEL == \"HRZN\":\n      !rm -rf url.txt\n      !hrzn login $Token\n      os.system(f\"hrzn tunnel http://localhost:{PORT} >> url.txt 2>&1 &\")\n      time.sleep(5)\n\n      with open('url.txt', 'r') as file:\n        public_url = file.read()\n        public_url = !grep -oE \"https://[a-zA-Z0-9.-]+\\.hrzn\\.run\" url.txt\n        public_url = public_url[0]\n\n\n    def play_notification_sound():\n        display(Audio(url='https://raw.githubusercontent.com/hinabl/rmvpe-ai-kaggle/main/custom/audios/notif.mp3', autoplay=True))\n\n\n    def wait_for_server():\n        while True:\n            time.sleep(0.5)\n            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n            result = sock.connect_ex(('127.0.0.1', PORT))\n            if result == 0:\n                break\n            sock.close()\n        if ClearConsole:\n            clear_output()\n        print(\"--------- SERVER READY! ---------\")\n        print(\"Your server is available at:\")\n        print(public_url)\n        print(\"---------------------------------\")\n        if Play_Notification==True:\n          play_notification_sound()\n\n    threading.Thread(target=wait_for_server, daemon=True).start()\n\n    mainpy=codecs.decode('ZZIPFreireFVB.cl','rot_13')\n    mainname=codecs.decode('ZZIPFreireFVB','rot_13')\n    !mv {mainpy} HVoice.py\n    !sed -i \"s/MMVCServerSIO/HVoice/\" HVoice.py\n    !python3 HVoice.py \\\n      -p {PORT} \\\n      --https False \\\n      --content_vec_500 pretrain/checkpoint_best_legacy_500.pt \\\n      --content_vec_500_onnx pretrain/content_vec_500.onnx \\\n      --content_vec_500_onnx_on false \\\n      --hubert_base pretrain/hubert_base.pt \\\n      --hubert_base_jp pretrain/rinna_hubert_base_jp.pt \\\n      --hubert_soft pretrain/hubert/hubert-soft-0d54a1f4.pt \\\n      --nsf_hifigan pretrain/nsf_hifigan/model \\\n      --crepe_onnx_full pretrain/crepe_onnx_full.onnx \\\n      --crepe_onnx_tiny pretrain/crepe_onnx_tiny.onnx \\\n      --rmvpe pretrain/rmvpe.pt \\\n      --model_dir model_dir \\\n      --samples samples.json \\\n      --allowed-origins {public_url}\n\n    if TUNNEL == \"NGROK\":\n      ngrok.disconnect(ngrokConnection.public_url)\n      print(\"--------- SERVER STOPPED! ---------\")\n    elif TUNNEL == \"HRZN\":\n      !rm -rf url.txt\n      !fuser -k ${PORT}\n      print(\"--------- SERVER STOPPED! ---------\")","metadata":{"id":"lLWQuUd7WW9U","cellView":"form"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"![](https://i.pinimg.com/474x/de/72/9e/de729ecfa41b69901c42c82fff752414.jpg)\n![](https://i.pinimg.com/474x/de/72/9e/de729ecfa41b69901c42c82fff752414.jpg)","metadata":{"id":"i8od_nC6kh1r"}}]}